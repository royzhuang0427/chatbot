{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化過程出錯: name '__file__' is not defined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from groq import Groq\n",
    "import google.generativeai as genai\n",
    "import openai\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"C:\\\\chatbot\\\\ENGLISHTEST\\\\.env\")\n",
    "if not os.getenv('GROQ_API_KEY'):\n",
    "    print(\"警告: GROQ_API_KEY 未設置\")\n",
    "groq_client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
    "\n",
    "sys_msg = (\n",
    "    \"You are an experienced English teacher specializing in TOEIC preparation. \"\n",
    "    \"Your responsibilities include: \"\n",
    "    \"1. Creating TOEIC-style questions when requested \"\n",
    "    \"2. Providing detailed explanations for answers \"\n",
    "    \"3. Teaching relevant grammar points and vocabulary \"\n",
    "    \"4. Giving study tips and strategies \"\n",
    "    \"5. Correcting student's English mistakes \"\n",
    "    \"6. Providing detailed explanations for answers \"\n",
    "    \"7. Teaching relevant grammar points and vocabulary \"\n",
    "    \"8. Giving study tips and strategies \"\n",
    "    \"9. Correcting student's English mistakes \"\n",
    "    \"Please ensure all responses are in proper English. \"\n",
    "    \"When explaining, be thorough but easy to understand. \"\n",
    "    \"Always maintain a supportive and encouraging teaching tone.\"\n",
    ")\n",
    "\n",
    "convo = [{'role': 'system', 'content': sys_msg}]\n",
    "\n",
    "generation_config = {\n",
    "    'temperature': 0.95,\n",
    "    'top_p': 1,\n",
    "    'top_k': 5,\n",
    "    'max_output_tokens': 2048\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    {'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_NONE'},\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest',\n",
    "                              generation_config=generation_config,\n",
    "                              safety_settings=safety_settings)\n",
    "\n",
    "def groq_prompt(prompt):\n",
    "    convo.append({'role': 'user', 'content': prompt})\n",
    "    try:\n",
    "        chat_completion = groq_client.chat.completions.create(messages=convo, model='llama3-70b-8192')\n",
    "        response = chat_completion.choices[0].message\n",
    "        convo.append(response)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Groq API: {e}\")\n",
    "        return \"系統出現錯誤\"\n",
    "\n",
    "\n",
    "\n",
    "folder_path = \"C:\\\\chatbot\\\\ENGLISHTEST\\\\result\"\n",
    "output_txt_path = \"C:\\\\chatbot\\\\ENGLISHTEST\\\\result.txt\"\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + \"\\n\" \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"讀取PDF時出錯 {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def read_pdfs_in_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"錯誤: 文件夾不存在 {folder_path}\")\n",
    "        return []\n",
    "    all_texts = []\n",
    "    \n",
    "    try:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.pdf'):\n",
    "                    pdf_path = os.path.join(root, file)\n",
    "                    print(f\"正在讀取PDF：{pdf_path}\")\n",
    "                    text = read_pdf(pdf_path)\n",
    "                    if text:\n",
    "                        all_texts.append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件夾時出錯: {str(e)}\")\n",
    "    \n",
    "    return all_texts\n",
    "\n",
    "def save_sourse_in_txt(texts, output_txt_path):\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as f:\n",
    "        for i, text in enumerate(texts, 1):\n",
    "            f.write(f\"--- PDF {i} ---\\n\")\n",
    "            f.write(text)\n",
    "            f.write(\"\\n\\n\") \n",
    "    # print(f\"所有文字已儲存至 {output_txt_path}\")\n",
    "\n",
    "\n",
    "embed_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "def generate_embedding(text):\n",
    "    return embed_model.encode(text)\n",
    "\n",
    "def reembedding():\n",
    "    try:\n",
    "        print(\"開始重新embedding...\")\n",
    "        vector_db.clear_all()\n",
    "        \n",
    "        pdf_texts = read_pdfs_in_folder(folder_path)\n",
    "        if not pdf_texts:\n",
    "            print(\"警告: 沒有讀取到任何PDF文本\")\n",
    "            return\n",
    "        save_sourse_in_txt(pdf_texts, output_txt_path)\n",
    "        \n",
    "        for text in pdf_texts:\n",
    "            if vector_db.add_text(text):\n",
    "                print(\"添加了新文本\")\n",
    "        print(\"重新embedding完成\")\n",
    "    except Exception as e:\n",
    "        print(f\"重新embedding時出錯: {str(e)}\")\n",
    "\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, dimension=384, index_file=\"vector_index.faiss\"):\n",
    "        self.dimension = dimension\n",
    "        self.index_file = os.path.join(os.path.dirname(__file__), index_file)\n",
    "        self.text_hash_file = os.path.join(os.path.dirname(__file__), \"text_hash_file.txt\")\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.texts = []\n",
    "        self.text_hashes = set() \n",
    "        self._load_index()\n",
    "        self._load_text_hashes()\n",
    "\n",
    "    def _load_text_hashes(self):\n",
    "        if os.path.exists(self.text_hash_file):\n",
    "            with open(self.text_hash_file, 'r') as f:\n",
    "                self.text_hashes = set(f.read().splitlines())\n",
    "\n",
    "    def _save_text_hashes(self):\n",
    "        with open(self.text_hash_file, 'w') as f:\n",
    "            f.write('\\n'.join(self.text_hashes))\n",
    "\n",
    "    def _load_index(self):\n",
    "        if os.path.exists(self.index_file):\n",
    "            try:\n",
    "                self.index = faiss.read_index(self.index_file) \n",
    "                # print(f\"Loaded existing index from {self.index_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading index: {e}\")\n",
    "        else:\n",
    "            print(f\"No existing index found. Creating a new one.\")\n",
    "            self.index = faiss.IndexFlatL2(self.dimension)\n",
    "            self.texts = [] \n",
    "\n",
    "    def _save_index(self):\n",
    "        faiss.write_index(self.index, self.index_file)\n",
    "        # print(f\"Index saved to {self.index_file}\")\n",
    "\n",
    "    def add_text(self, text, metadata=None):\n",
    "        text_hash = str(hash(text))\n",
    "        if text_hash in self.text_hashes:\n",
    "            return False\n",
    "        embedding = generate_embedding(text)\n",
    "        embedding = np.array(embedding, dtype=np.float32)\n",
    "        self.index.add(np.array([embedding], dtype=np.float32))\n",
    "        self.texts.append(text)\n",
    "        self.text_hashes.add(text_hash)\n",
    "        self._save_text_hashes()\n",
    "        self._save_index()\n",
    "        return True\n",
    "\n",
    "    def search(self, query, top_k=5):\n",
    "        query_embedding = generate_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding], dtype=np.float32), top_k)\n",
    "        return distances, indices\n",
    "    \n",
    "    def get_text_by_index(self, index):\n",
    "        try:\n",
    "            return self.texts[index]\n",
    "        except IndexError:\n",
    "            return \"未找到相關文本\"\n",
    "        \n",
    "    def _save_index(self):\n",
    "        try:\n",
    "            faiss.write_index(self.index, self.index_file)\n",
    "            print(f\"索引已保存到 {self.index_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"保存索引時出錯: {e}\")    \n",
    "        \n",
    "    def clear_all(self):\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.texts = []\n",
    "        self.text_hashes = set()\n",
    "        if os.path.exists(self.index_file):\n",
    "            os.remove(self.index_file)\n",
    "        if os.path.exists(self.text_hash_file):\n",
    "            os.remove(self.text_hash_file)\n",
    "        print(\"已清除所有向量数据\")\n",
    "\n",
    "    def close(self):\n",
    "        self._save_index()\n",
    "        self._save_text_hashes()\n",
    "\n",
    "\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    sentences = text.split('.') \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        sentence_length = len(sentence.split())\n",
    "        if current_length + sentence_length <= max_length:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def process_text_and_generate_embeddings(text):\n",
    "    chunks = chunk_text(text) \n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embedding = generate_embedding(chunk)\n",
    "        embeddings.append(embedding)\n",
    "        # print(f\"生成的嵌入: {embedding}\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_text_from_db(query, top_k=3):\n",
    "    distances, indices = vector_db.search(query, top_k=top_k)\n",
    "    # 限制返回文本的長度\n",
    "    relevant_text = vector_db.get_text_by_index(0)\n",
    "    # 限制文本長度為 2000 字符\n",
    "    return relevant_text[:2000] if relevant_text else \"\"\n",
    "\n",
    "def generate_response_with_context(query):\n",
    "    relevant_texts = get_relevant_text_from_db(query)\n",
    "    prompt = (\n",
    "        f\"As an English teacher, please help with the following request while considering \"\n",
    "        f\"this reference material: {relevant_texts}\\n\\n\"\n",
    "        f\"Student's request: {query}\\n\\n\"\n",
    "        f\"Please provide a comprehensive response that includes:\\n\"\n",
    "        f\"1. Direct answer to the student's question\\n\"\n",
    "        f\"2. Relevant explanations or teaching points\\n\"\n",
    "        f\"3. Examples if applicable\\n\"\n",
    "        f\"4. Study tips or suggestions when appropriate\"\n",
    "    )\n",
    "    try:\n",
    "        response = groq_prompt(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"API 錯誤: {e}\")\n",
    "        return groq_prompt(query)\n",
    "    \n",
    "def save_response_to_txt(query, response, output_file=\"responses.txt\"):\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    if not os.path.exists(output_file):\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# GenAI 回應記錄\\n\\n\")\n",
    "    with open(output_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"\\n{'='*50}\\n\")\n",
    "        f.write(f\"用戶提問: {query}\\n\") \n",
    "        f.write(f\"{'='*50}\\n\")\n",
    "        f.write(response)\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "try:\n",
    "    vector_db = VectorDB()\n",
    "    pdf_texts = read_pdfs_in_folder(folder_path)\n",
    "    if pdf_texts:\n",
    "        save_sourse_in_txt(pdf_texts, output_txt_path)\n",
    "        for text in pdf_texts:\n",
    "            if vector_db.add_text(text):\n",
    "                print(\"添加了新文本\")\n",
    "            else:\n",
    "                print(\"文本已存在，跳過處理\")\n",
    "    else:\n",
    "        print(\"警告: 沒有找到PDF文件或文件讀取失敗\")\n",
    "except Exception as e:\n",
    "    print(f\"初始化過程出錯: {str(e)}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"請輸入您的問題（輸入'quit'退出）: \")\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "            elif query.lower() == 'reembedding':\n",
    "                reembedding()\n",
    "                continue\n",
    "            \n",
    "            response = generate_response_with_context(query)\n",
    "            save_response_to_txt(query, response)\n",
    "            print(\"\\nGenAI 回覆:\", response)\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"處理查詢時出錯: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    vector_db.close()\n",
    "    print(\"程序結束，索引已保存\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
